{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf6defa-447d-4c31-8076-5ec90e5363c0",
   "metadata": {},
   "source": [
    "## Step 1 : Load and Prepare The Dataset ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef98f728-aaa3-4317-b6e9-f5de16f696b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1833e3d2-1e42-4d43-88d5-d41728fcfb27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d65cd7fb-8225-4cd5-9c58-74619befffa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a71d2e-3ed8-4db2-86fa-c7310d2b47c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1399eea1-475f-49c0-b5f2-8bacb6c7ca46",
   "metadata": {},
   "source": [
    "## Step 2 : Process The Data ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d274cfe-ebea-4314-9141-8ff5b95366a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40bdc540c0a543cea361ac4d6d220503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load The Tokenizer ?\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize The Dataset ?\n",
    "def tokenize_function (examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99014f08-7a69-4d54-a6bb-b83851e6c5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8971ee99-d6df-4d02-8d09-f7096cb3af39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0,\n",
       " 'input_ids': [101,\n",
       "  1045,\n",
       "  12524,\n",
       "  1045,\n",
       "  2572,\n",
       "  8025,\n",
       "  1011,\n",
       "  3756,\n",
       "  2013,\n",
       "  2026,\n",
       "  2678,\n",
       "  3573,\n",
       "  2138,\n",
       "  1997,\n",
       "  2035,\n",
       "  1996,\n",
       "  6704,\n",
       "  2008,\n",
       "  5129,\n",
       "  2009,\n",
       "  2043,\n",
       "  2009,\n",
       "  2001,\n",
       "  2034,\n",
       "  2207,\n",
       "  1999,\n",
       "  3476,\n",
       "  1012,\n",
       "  1045,\n",
       "  2036,\n",
       "  2657,\n",
       "  2008,\n",
       "  2012,\n",
       "  2034,\n",
       "  2009,\n",
       "  2001,\n",
       "  8243,\n",
       "  2011,\n",
       "  1057,\n",
       "  1012,\n",
       "  1055,\n",
       "  1012,\n",
       "  8205,\n",
       "  2065,\n",
       "  2009,\n",
       "  2412,\n",
       "  2699,\n",
       "  2000,\n",
       "  4607,\n",
       "  2023,\n",
       "  2406,\n",
       "  1010,\n",
       "  3568,\n",
       "  2108,\n",
       "  1037,\n",
       "  5470,\n",
       "  1997,\n",
       "  3152,\n",
       "  2641,\n",
       "  1000,\n",
       "  6801,\n",
       "  1000,\n",
       "  1045,\n",
       "  2428,\n",
       "  2018,\n",
       "  2000,\n",
       "  2156,\n",
       "  2023,\n",
       "  2005,\n",
       "  2870,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1996,\n",
       "  5436,\n",
       "  2003,\n",
       "  8857,\n",
       "  2105,\n",
       "  1037,\n",
       "  2402,\n",
       "  4467,\n",
       "  3689,\n",
       "  3076,\n",
       "  2315,\n",
       "  14229,\n",
       "  2040,\n",
       "  4122,\n",
       "  2000,\n",
       "  4553,\n",
       "  2673,\n",
       "  2016,\n",
       "  2064,\n",
       "  2055,\n",
       "  2166,\n",
       "  1012,\n",
       "  1999,\n",
       "  3327,\n",
       "  2016,\n",
       "  4122,\n",
       "  2000,\n",
       "  3579,\n",
       "  2014,\n",
       "  3086,\n",
       "  2015,\n",
       "  2000,\n",
       "  2437,\n",
       "  2070,\n",
       "  4066,\n",
       "  1997,\n",
       "  4516,\n",
       "  2006,\n",
       "  2054,\n",
       "  1996,\n",
       "  2779,\n",
       "  25430,\n",
       "  14728,\n",
       "  2245,\n",
       "  2055,\n",
       "  3056,\n",
       "  2576,\n",
       "  3314,\n",
       "  2107,\n",
       "  2004,\n",
       "  1996,\n",
       "  5148,\n",
       "  2162,\n",
       "  1998,\n",
       "  2679,\n",
       "  3314,\n",
       "  1999,\n",
       "  1996,\n",
       "  2142,\n",
       "  2163,\n",
       "  1012,\n",
       "  1999,\n",
       "  2090,\n",
       "  4851,\n",
       "  8801,\n",
       "  1998,\n",
       "  6623,\n",
       "  7939,\n",
       "  4697,\n",
       "  3619,\n",
       "  1997,\n",
       "  8947,\n",
       "  2055,\n",
       "  2037,\n",
       "  10740,\n",
       "  2006,\n",
       "  4331,\n",
       "  1010,\n",
       "  2016,\n",
       "  2038,\n",
       "  3348,\n",
       "  2007,\n",
       "  2014,\n",
       "  3689,\n",
       "  3836,\n",
       "  1010,\n",
       "  19846,\n",
       "  1010,\n",
       "  1998,\n",
       "  2496,\n",
       "  2273,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  2054,\n",
       "  8563,\n",
       "  2033,\n",
       "  2055,\n",
       "  1045,\n",
       "  2572,\n",
       "  8025,\n",
       "  1011,\n",
       "  3756,\n",
       "  2003,\n",
       "  2008,\n",
       "  2871,\n",
       "  2086,\n",
       "  3283,\n",
       "  1010,\n",
       "  2023,\n",
       "  2001,\n",
       "  2641,\n",
       "  26932,\n",
       "  1012,\n",
       "  2428,\n",
       "  1010,\n",
       "  1996,\n",
       "  3348,\n",
       "  1998,\n",
       "  16371,\n",
       "  25469,\n",
       "  5019,\n",
       "  2024,\n",
       "  2261,\n",
       "  1998,\n",
       "  2521,\n",
       "  2090,\n",
       "  1010,\n",
       "  2130,\n",
       "  2059,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  2025,\n",
       "  2915,\n",
       "  2066,\n",
       "  2070,\n",
       "  10036,\n",
       "  2135,\n",
       "  2081,\n",
       "  22555,\n",
       "  2080,\n",
       "  1012,\n",
       "  2096,\n",
       "  2026,\n",
       "  2406,\n",
       "  3549,\n",
       "  2568,\n",
       "  2424,\n",
       "  2009,\n",
       "  16880,\n",
       "  1010,\n",
       "  1999,\n",
       "  4507,\n",
       "  3348,\n",
       "  1998,\n",
       "  16371,\n",
       "  25469,\n",
       "  2024,\n",
       "  1037,\n",
       "  2350,\n",
       "  18785,\n",
       "  1999,\n",
       "  4467,\n",
       "  5988,\n",
       "  1012,\n",
       "  2130,\n",
       "  13749,\n",
       "  7849,\n",
       "  24544,\n",
       "  1010,\n",
       "  15835,\n",
       "  2037,\n",
       "  3437,\n",
       "  2000,\n",
       "  2204,\n",
       "  2214,\n",
       "  2879,\n",
       "  2198,\n",
       "  4811,\n",
       "  1010,\n",
       "  2018,\n",
       "  3348,\n",
       "  5019,\n",
       "  1999,\n",
       "  2010,\n",
       "  3152,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1045,\n",
       "  2079,\n",
       "  4012,\n",
       "  3549,\n",
       "  2094,\n",
       "  1996,\n",
       "  16587,\n",
       "  2005,\n",
       "  1996,\n",
       "  2755,\n",
       "  2008,\n",
       "  2151,\n",
       "  3348,\n",
       "  3491,\n",
       "  1999,\n",
       "  1996,\n",
       "  2143,\n",
       "  2003,\n",
       "  3491,\n",
       "  2005,\n",
       "  6018,\n",
       "  5682,\n",
       "  2738,\n",
       "  2084,\n",
       "  2074,\n",
       "  2000,\n",
       "  5213,\n",
       "  2111,\n",
       "  1998,\n",
       "  2191,\n",
       "  2769,\n",
       "  2000,\n",
       "  2022,\n",
       "  3491,\n",
       "  1999,\n",
       "  26932,\n",
       "  12370,\n",
       "  1999,\n",
       "  2637,\n",
       "  1012,\n",
       "  1045,\n",
       "  2572,\n",
       "  8025,\n",
       "  1011,\n",
       "  3756,\n",
       "  2003,\n",
       "  1037,\n",
       "  2204,\n",
       "  2143,\n",
       "  2005,\n",
       "  3087,\n",
       "  5782,\n",
       "  2000,\n",
       "  2817,\n",
       "  1996,\n",
       "  6240,\n",
       "  1998,\n",
       "  14629,\n",
       "  1006,\n",
       "  2053,\n",
       "  26136,\n",
       "  3832,\n",
       "  1007,\n",
       "  1997,\n",
       "  4467,\n",
       "  5988,\n",
       "  1012,\n",
       "  2021,\n",
       "  2428,\n",
       "  1010,\n",
       "  2023,\n",
       "  2143,\n",
       "  2987,\n",
       "  1005,\n",
       "  1056,\n",
       "  2031,\n",
       "  2172,\n",
       "  1997,\n",
       "  1037,\n",
       "  5436,\n",
       "  1012,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b65937-72b6-44fe-a0be-1da7771c99d1",
   "metadata": {},
   "source": [
    "## Step 3 : Set Up Training Data ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a1d5cad-d549-4cb5-9d7c-33cd29a78ea0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=0,\n",
       "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "average_tokens_across_devices=False,\n",
       "batch_eval_metrics=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_persistent_workers=False,\n",
       "dataloader_pin_memory=True,\n",
       "dataloader_prefetch_factor=None,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_do_concat_batches=True,\n",
       "eval_on_start=False,\n",
       "eval_steps=None,\n",
       "eval_strategy=IntervalStrategy.EPOCH,\n",
       "eval_use_gather_object=False,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "gradient_checkpointing_kwargs=None,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=None,\n",
       "hub_strategy=HubStrategy.EVERY_SAVE,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_for_metrics=[],\n",
       "include_inputs_for_metrics=False,\n",
       "include_num_input_tokens_seen=False,\n",
       "include_tokens_per_second=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=2e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=./results/res1\\runs\\Jun10_04-10-11_ShwaTech,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=IntervalStrategy.STEPS,\n",
       "lr_scheduler_kwargs={},\n",
       "lr_scheduler_type=SchedulerType.LINEAR,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "neftune_noise_alpha=None,\n",
       "no_cuda=False,\n",
       "num_train_epochs=3,\n",
       "optim=OptimizerNames.ADAMW_TORCH,\n",
       "optim_args=None,\n",
       "optim_target_modules=None,\n",
       "output_dir=./results/res1,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=16,\n",
       "per_device_train_batch_size=16,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=['tensorboard'],\n",
       "restore_callback_states_from_checkpoint=False,\n",
       "resume_from_checkpoint=None,\n",
       "run_name=./results/res1,\n",
       "save_on_each_node=False,\n",
       "save_only_model=False,\n",
       "save_safetensors=True,\n",
       "save_steps=500,\n",
       "save_strategy=SaveStrategy.STEPS,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torch_empty_cache_steps=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_cpu=False,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_liger_kernel=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.01,\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f08a60e-c980-46d2-9ce3-c63332305084",
   "metadata": {},
   "source": [
    "## Step 4 : Initialize The Model ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0e3e91d-23d7-4961-ad64-6a1cfc80bbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf83c05fb8194d629b3472eecff001f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hp\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer\n",
    "\n",
    "# Load The Pre-Trained Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Initialize The Trainer ??\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee0e520-df1b-4bc9-b936-67e001f887c8",
   "metadata": {},
   "source": [
    "## Step 6 : Train The Model ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fabfe230-fc6c-4179-ab11-b177f9089efa",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2240\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2238\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2241\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2242\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2243\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2244\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2245\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2555\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2548\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2549\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2551\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2553\u001b[0m )\n\u001b[0;32m   2554\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2555\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2558\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2561\u001b[0m ):\n\u001b[0;32m   2562\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2563\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3791\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   3789\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 3791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3793\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:2473\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2471\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2473\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d4c4d9-997f-4933-8f33-f98b325ac8d3",
   "metadata": {},
   "source": [
    "## Step 7 : Evaluate The Model ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca3fc80-7e5e-4342-8235-f3d47831cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02b1372-e21e-4da7-9d4c-5000cd2a228a",
   "metadata": {},
   "source": [
    "## Step 8 : Save The Fine-Tuned Model ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a46a7-e8bc-4911-b30a-a45cedc94801",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./fine-tuned-model')\n",
    "\n",
    "tokenizer.save_pretrained('./fine-tuned-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865e91c4-c0a4-44b1-90a2-6d215851fca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f0329a6-832a-4ffd-9ea1-7a208533611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import arxiv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f93e9313-3661-4ceb-aa96-eb453d76868a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_8548\\2565829634.py:9: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-06 17:59:50+00:00</td>\n",
       "      <td>TerraFM: A Scalable Foundation Model for Unified Multisensor Earth Observation</td>\n",
       "      <td>Modern Earth observation (EO) increasingly leverages deep learning to harness\\nthe scale and diversity of satellite imagery across sensors and regions. While\\nrecent foundation models have demonstrated promising generalization across EO\\ntasks, many remain limited by the scale, geographical coverage, and spectral\\ndiversity of their training data, factors critical for learning globally\\ntransferable representations. In this work, we introduce TerraFM, a scalable\\nself-supervised learning model that leverages globally distributed Sentinel-1\\nand Sentinel-2 imagery, combined with large spatial tiles and land-cover aware\\nsampling to enrich spatial and semantic coverage. By treating sensing\\nmodalities as natural augmentations in our self-supervised approach, we unify\\nradar and optical inputs via modality-specific patch embeddings and adaptive\\ncross-attention fusion. Our training strategy integrates local-global\\ncontrastive learning and introduces a dual-centering mechanism that\\nincorporates class-frequency-aware regularization to address long-tailed\\ndistributions in land cover.TerraFM achieves strong generalization on both\\nclassification and segmentation tasks, outperforming prior models on GEO-Bench\\nand Copernicus-Bench. Our code and pretrained models are publicly available at:\\nhttps://github.com/mbzuai-oryx/TerraFM .</td>\n",
       "      <td>[cs.CV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-06-06 17:59:28+00:00</td>\n",
       "      <td>Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias</td>\n",
       "      <td>Diagnosing deep neural networks (DNNs) through the eigenspectrum of weight\\nmatrices has been an active area of research in recent years. At a high level,\\neigenspectrum analysis of DNNs involves measuring the heavytailness of the\\nempirical spectral densities (ESD) of weight matrices. It provides insight into\\nhow well a model is trained and can guide decisions on assigning better\\nlayer-wise training hyperparameters. In this paper, we address a challenge\\nassociated with such eigenspectrum methods: the impact of the aspect ratio of\\nweight matrices on estimated heavytailness metrics. We demonstrate that\\nmatrices of varying sizes (and aspect ratios) introduce a non-negligible bias\\nin estimating heavytailness metrics, leading to inaccurate model diagnosis and\\nlayer-wise hyperparameter assignment. To overcome this challenge, we propose\\nFARMS (Fixed-Aspect-Ratio Matrix Subsampling), a method that normalizes the\\nweight matrices by subsampling submatrices with a fixed aspect ratio. Instead\\nof measuring the heavytailness of the original ESD, we measure the average ESD\\nof these subsampled submatrices. We show that measuring the heavytailness of\\nthese submatrices with the fixed aspect ratio can effectively mitigate the\\naspect ratio bias. We validate our approach across various optimization\\ntechniques and application domains that involve eigenspectrum analysis of\\nweights, including image classification in computer vision (CV) models,\\nscientific machine learning (SciML) model training, and large language model\\n(LLM) pruning. Our results show that despite its simplicity, FARMS uniformly\\nimproves the accuracy of eigenspectrum analysis while enabling more effective\\nlayer-wise hyperparameter assignment in these application domains. In one of\\nthe LLM pruning experiments, FARMS reduces the perplexity of the LLaMA-7B model\\nby 17.3% when compared with the state-of-the-art method.</td>\n",
       "      <td>[cs.LG, cs.AI]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-06-06 17:58:54+00:00</td>\n",
       "      <td>Distillation Robustifies Unlearning</td>\n",
       "      <td>Current LLM unlearning methods are not robust: they can be reverted easily\\nwith a few steps of finetuning. This is true even for the idealized unlearning\\nmethod of training to imitate an oracle model that was never exposed to\\nunwanted information, suggesting that output-based finetuning is insufficient\\nto achieve robust unlearning. In a similar vein, we find that training a\\nrandomly initialized student to imitate an unlearned model transfers desired\\nbehaviors while leaving undesired capabilities behind. In other words,\\ndistillation robustifies unlearning. Building on this insight, we propose\\nUnlearn-Noise-Distill-on-Outputs (UNDO), a scalable method that distills an\\nunlearned model into a partially noised copy of itself. UNDO introduces a\\ntunable tradeoff between compute cost and robustness, establishing a new Pareto\\nfrontier on synthetic language and arithmetic tasks. At its strongest setting,\\nUNDO matches the robustness of a model retrained from scratch with perfect data\\nfiltering while using only 60-80% of the compute and requiring only 0.01% of\\nthe pretraining data to be labeled. We also show that UNDO robustifies\\nunlearning on the more realistic Weapons of Mass Destruction Proxy (WMDP)\\nbenchmark. Since distillation is widely used in practice, incorporating an\\nunlearning step beforehand offers a convenient path to robust capability\\nremoval.</td>\n",
       "      <td>[cs.LG, cs.AI]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-06-06 17:58:36+00:00</td>\n",
       "      <td>Movie Facts and Fibs (MF$^2$): A Benchmark for Long Movie Understanding</td>\n",
       "      <td>Despite recent progress in vision-language models (VLMs), holistic\\nunderstanding of long-form video content remains a significant challenge,\\npartly due to limitations in current benchmarks. Many focus on peripheral,\\n``needle-in-a-haystack'' details, encouraging context-insensitive retrieval\\nover deep comprehension. Others rely on large-scale, semi-automatically\\ngenerated questions (often produced by language models themselves) that are\\neasier for models to answer but fail to reflect genuine understanding. In this\\npaper, we introduce MF$^2$, a new benchmark for evaluating whether models can\\ncomprehend, consolidate, and recall key narrative information from full-length\\nmovies (50-170 minutes long). MF$^2$ includes over 50 full-length,\\nopen-licensed movies, each paired with manually constructed sets of claim pairs\\n-- one true (fact) and one plausible but false (fib), totalling over 850 pairs.\\nThese claims target core narrative elements such as character motivations and\\nemotions, causal chains, and event order, and refer to memorable moments that\\nhumans can recall without rewatching the movie. Instead of multiple-choice\\nformats, we adopt a binary claim evaluation protocol: for each pair, models\\nmust correctly identify both the true and false claims. This reduces biases\\nlike answer ordering and enables a more precise assessment of reasoning. Our\\nexperiments demonstrate that both open-weight and closed state-of-the-art\\nmodels fall well short of human performance, underscoring the relative ease of\\nthe task for humans and their superior ability to retain and reason over\\ncritical narrative information -- an ability current VLMs lack.</td>\n",
       "      <td>[cs.CV, cs.CL, cs.LG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-06-06 17:51:26+00:00</td>\n",
       "      <td>Accurately simulating core-collapse self-interacting dark matter halos</td>\n",
       "      <td>The properties of satellite halos provide a promising probe for dark matter\\n(DM) physics. Observations motivate current efforts to explain surprisingly\\ncompact DM halos. If DM is not collisionless but has strong self-interactions,\\nhalos can undergo gravothermal collapse, leading to higher densities in the\\ncentral region of the halo. However, it is challenging to model this collapse\\nphase from first principles. To improve on this, we seek to better understand\\nnumerical challenges and convergence properties of self-interacting dark matter\\n(SIDM) N-body simulations in the collapse phase. Especially we aim for a better\\nunderstanding of the evolution of satellite halos. To do so, we run SIDM N-body\\nsimulations of a low mass halo in isolation and within an external\\ngravitational potential. The simulation setup is motivated by the perturber of\\nthe stellar stream GD-1. We find that the halo evolution is very sensitive to\\nenergy conservation errors, and a too large SIDM kernel size can artificially\\nspeed up the collapse. Moreover, we demonstrate that the King model can\\ndescribe the density profile at small radii for the late stages that we have\\nsimulated. Furthermore, for our highest-resolved simulation (N = 5x10^7) we\\nmake the data public. It can serve as a benchmark. Overall, we find that the\\ncurrent numerical methods do not suffer from convergence problems in the late\\ncollapse phase and provide guidance on how to choose numerical parameters, e.g.\\nthat the energy conservation error is better kept well below 1%. This allows to\\nrun simulations of halos becoming concentrated enough to explain observations\\nof GD-1 like stellar streams or strong gravitational lensing systems.</td>\n",
       "      <td>[astro-ph.CO, astro-ph.GA, hep-ph]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-06-06 17:48:23+00:00</td>\n",
       "      <td>Cartridges: Lightweight and general-purpose long context representations via self-study</td>\n",
       "      <td>Large language models are often used to answer queries grounded in large text\\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\\nentire corpus in the context window and leveraging in-context learning (ICL).\\nAlthough current models support contexts of 100K-1M tokens, this setup is\\ncostly to serve because the memory consumption of the KV cache scales with\\ninput length. We explore an alternative: training a smaller KV cache offline on\\neach corpus. At inference time, we load this trained KV cache, which we call a\\nCartridge, and decode a response. Critically, the cost of training a Cartridge\\ncan be amortized across all the queries referencing the same corpus. However,\\nwe find that the naive approach of training the Cartridge with next-token\\nprediction on the corpus is not competitive with ICL. Instead, we propose\\nself-study, a training recipe in which we generate synthetic conversations\\nabout the corpus and train the Cartridge with a context-distillation objective.\\nWe find that Cartridges trained with self-study replicate the functionality of\\nICL, while being significantly cheaper to serve. On challenging long-context\\nbenchmarks, Cartridges trained with self-study match ICL performance while\\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\\nextends the model's effective context length (e.g. from 128k to 484k tokens on\\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\\ntime without retraining.</td>\n",
       "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-06-06 17:47:27+00:00</td>\n",
       "      <td>Integrating Complexity and Biological Realism: High-Performance Spiking Neural Networks for Breast Cancer Detection</td>\n",
       "      <td>Spiking Neural Networks (SNNs) event-driven nature enables efficient encoding\\nof spatial and temporal features, making them suitable for dynamic\\ntime-dependent data processing. Despite their biological relevance, SNNs have\\nseen limited application in medical image recognition due to difficulties in\\nmatching the performance of conventional deep learning models. To address this,\\nwe propose a novel breast cancer classification approach that combines SNNs\\nwith Lempel-Ziv Complexity (LZC) a computationally efficient measure of\\nsequence complexity. LZC enhances the interpretability and accuracy of\\nspike-based models by capturing structural patterns in neural activity. Our\\nstudy explores both biophysical Leaky Integrate-and-Fire (LIF) and\\nprobabilistic Levy-Baxter (LB) neuron models under supervised, unsupervised,\\nand hybrid learning regimes. Experiments were conducted on the Breast Cancer\\nWisconsin dataset using numerical features derived from medical imaging.\\nLB-based models consistently exceeded 90.00% accuracy, while LIF-based models\\nreached over 85.00%. The highest accuracy of 98.25% was achieved using an\\nANN-to-SNN conversion method applied to both neuron models comparable to\\ntraditional deep learning with back-propagation, but at up to 100 times lower\\ncomputational cost. This hybrid approach merges deep learning performance with\\nthe efficiency and plausibility of SNNs, yielding top results at lower\\ncomputational cost. We hypothesize that the synergy between temporal-coding,\\nspike-sparsity, and LZC-driven complexity analysis enables more-efficient\\nfeature extraction. Our findings demonstrate that SNNs combined with LZC offer\\npromising, biologically plausible alternative to conventional neural networks\\nin medical diagnostics, particularly for resource-constrained or real-time\\nsystems.</td>\n",
       "      <td>[cs.NE, eess.IV, q-bio.NC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-06-06 17:43:00+00:00</td>\n",
       "      <td>PyGemini: Unified Software Development towards Maritime Autonomy Systems</td>\n",
       "      <td>Ensuring the safety and certifiability of autonomous surface vessels (ASVs)\\nrequires robust decision-making systems, supported by extensive simulation,\\ntesting, and validation across a broad range of scenarios. However, the current\\nlandscape of maritime autonomy development is fragmented -- relying on\\ndisparate tools for communication, simulation, monitoring, and system\\nintegration -- which hampers interdisciplinary collaboration and inhibits the\\ncreation of compelling assurance cases, demanded by insurers and regulatory\\nbodies. Furthermore, these disjointed tools often suffer from performance\\nbottlenecks, vendor lock-in, and limited support for continuous integration\\nworkflows. To address these challenges, we introduce PyGemini, a permissively\\nlicensed, Python-native framework that builds on the legacy of Autoferry Gemini\\nto unify maritime autonomy development. PyGemini introduces a novel\\nConfiguration-Driven Development (CDD) process that fuses Behavior-Driven\\nDevelopment (BDD), data-oriented design, and containerization to support\\nmodular, maintainable, and scalable software architectures. The framework\\nfunctions as a stand-alone application, cloud-based service, or embedded\\nlibrary -- ensuring flexibility across research and operational contexts. We\\ndemonstrate its versatility through a suite of maritime tools -- including 3D\\ncontent generation for simulation and monitoring, scenario generation for\\nautonomy validation and training, and generative artificial intelligence\\npipelines for augmenting imagery -- thereby offering a scalable, maintainable,\\nand performance-oriented foundation for future maritime robotics and autonomy\\nresearch.</td>\n",
       "      <td>[cs.RO, cs.SE, cs.SY, eess.SY, D.2.11; I.6.2; I.2.9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-06-06 17:40:12+00:00</td>\n",
       "      <td>Reflect-then-Plan: Offline Model-Based Planning through a Doubly Bayesian Lens</td>\n",
       "      <td>Offline reinforcement learning (RL) is crucial when online exploration is\\ncostly or unsafe but often struggles with high epistemic uncertainty due to\\nlimited data. Existing methods rely on fixed conservative policies, restricting\\nadaptivity and generalization. To address this, we propose Reflect-then-Plan\\n(RefPlan), a novel doubly Bayesian offline model-based (MB) planning approach.\\nRefPlan unifies uncertainty modeling and MB planning by recasting planning as\\nBayesian posterior estimation. At deployment, it updates a belief over\\nenvironment dynamics using real-time observations, incorporating uncertainty\\ninto MB planning via marginalization. Empirical results on standard benchmarks\\nshow that RefPlan significantly improves the performance of conservative\\noffline RL policies. In particular, RefPlan maintains robust performance under\\nhigh epistemic uncertainty and limited data, while demonstrating resilience to\\nchanging environment dynamics, improving the flexibility, generalizability, and\\nrobustness of offline-learned policies.</td>\n",
       "      <td>[cs.AI, cs.LG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-06-06 17:39:32+00:00</td>\n",
       "      <td>An Optimized Franz-Parisi Criterion and its Equivalence with SQ Lower Bounds</td>\n",
       "      <td>Bandeira et al. (2022) introduced the Franz-Parisi (FP) criterion for\\ncharacterizing the computational hard phases in statistical detection problems.\\nThe FP criterion, based on an annealed version of the celebrated Franz-Parisi\\npotential from statistical physics, was shown to be equivalent to low-degree\\npolynomial (LDP) lower bounds for Gaussian additive models, thereby connecting\\ntwo distinct approaches to understanding the computational hardness in\\nstatistical inference. In this paper, we propose a refined FP criterion that\\naims to better capture the geometric ``overlap\" structure of statistical\\nmodels. Our main result establishes that this optimized FP criterion is\\nequivalent to Statistical Query (SQ) lower bounds -- another foundational\\nframework in computational complexity of statistical inference. Crucially, this\\nequivalence holds under a mild, verifiable assumption satisfied by a broad\\nclass of statistical models, including Gaussian additive models, planted sparse\\nmodels, as well as non-Gaussian component analysis (NGCA), single-index (SI)\\nmodels, and convex truncation detection settings. For instance, in the case of\\nconvex truncation tasks, the assumption is equivalent with the Gaussian\\ncorrelation inequality (Royen, 2014) from convex geometry.\\n  In addition to the above, our equivalence not only unifies and simplifies the\\nderivation of several known SQ lower bounds -- such as for the NGCA model\\n(Diakonikolas et al., 2017) and the SI model (Damian et al., 2024) -- but also\\nyields new SQ lower bounds of independent interest, including for the\\ncomputational gaps in mixed sparse linear regression (Arpino et al., 2023) and\\nconvex truncation (De et al., 2023).</td>\n",
       "      <td>[math.ST, cond-mat.stat-mech, cs.CC, stat.ML, stat.TH]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  published  \\\n",
       "0 2025-06-06 17:59:50+00:00   \n",
       "1 2025-06-06 17:59:28+00:00   \n",
       "2 2025-06-06 17:58:54+00:00   \n",
       "3 2025-06-06 17:58:36+00:00   \n",
       "4 2025-06-06 17:51:26+00:00   \n",
       "5 2025-06-06 17:48:23+00:00   \n",
       "6 2025-06-06 17:47:27+00:00   \n",
       "7 2025-06-06 17:43:00+00:00   \n",
       "8 2025-06-06 17:40:12+00:00   \n",
       "9 2025-06-06 17:39:32+00:00   \n",
       "\n",
       "                                                                                                                 title  \\\n",
       "0                                       TerraFM: A Scalable Foundation Model for Unified Multisensor Earth Observation   \n",
       "1                                                  Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias   \n",
       "2                                                                                  Distillation Robustifies Unlearning   \n",
       "3                                              Movie Facts and Fibs (MF$^2$): A Benchmark for Long Movie Understanding   \n",
       "4                                               Accurately simulating core-collapse self-interacting dark matter halos   \n",
       "5                              Cartridges: Lightweight and general-purpose long context representations via self-study   \n",
       "6  Integrating Complexity and Biological Realism: High-Performance Spiking Neural Networks for Breast Cancer Detection   \n",
       "7                                             PyGemini: Unified Software Development towards Maritime Autonomy Systems   \n",
       "8                                       Reflect-then-Plan: Offline Model-Based Planning through a Doubly Bayesian Lens   \n",
       "9                                         An Optimized Franz-Parisi Criterion and its Equivalence with SQ Lower Bounds   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   abstract  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Modern Earth observation (EO) increasingly leverages deep learning to harness\\nthe scale and diversity of satellite imagery across sensors and regions. While\\nrecent foundation models have demonstrated promising generalization across EO\\ntasks, many remain limited by the scale, geographical coverage, and spectral\\ndiversity of their training data, factors critical for learning globally\\ntransferable representations. In this work, we introduce TerraFM, a scalable\\nself-supervised learning model that leverages globally distributed Sentinel-1\\nand Sentinel-2 imagery, combined with large spatial tiles and land-cover aware\\nsampling to enrich spatial and semantic coverage. By treating sensing\\nmodalities as natural augmentations in our self-supervised approach, we unify\\nradar and optical inputs via modality-specific patch embeddings and adaptive\\ncross-attention fusion. Our training strategy integrates local-global\\ncontrastive learning and introduces a dual-centering mechanism that\\nincorporates class-frequency-aware regularization to address long-tailed\\ndistributions in land cover.TerraFM achieves strong generalization on both\\nclassification and segmentation tasks, outperforming prior models on GEO-Bench\\nand Copernicus-Bench. Our code and pretrained models are publicly available at:\\nhttps://github.com/mbzuai-oryx/TerraFM .   \n",
       "1  Diagnosing deep neural networks (DNNs) through the eigenspectrum of weight\\nmatrices has been an active area of research in recent years. At a high level,\\neigenspectrum analysis of DNNs involves measuring the heavytailness of the\\nempirical spectral densities (ESD) of weight matrices. It provides insight into\\nhow well a model is trained and can guide decisions on assigning better\\nlayer-wise training hyperparameters. In this paper, we address a challenge\\nassociated with such eigenspectrum methods: the impact of the aspect ratio of\\nweight matrices on estimated heavytailness metrics. We demonstrate that\\nmatrices of varying sizes (and aspect ratios) introduce a non-negligible bias\\nin estimating heavytailness metrics, leading to inaccurate model diagnosis and\\nlayer-wise hyperparameter assignment. To overcome this challenge, we propose\\nFARMS (Fixed-Aspect-Ratio Matrix Subsampling), a method that normalizes the\\nweight matrices by subsampling submatrices with a fixed aspect ratio. Instead\\nof measuring the heavytailness of the original ESD, we measure the average ESD\\nof these subsampled submatrices. We show that measuring the heavytailness of\\nthese submatrices with the fixed aspect ratio can effectively mitigate the\\naspect ratio bias. We validate our approach across various optimization\\ntechniques and application domains that involve eigenspectrum analysis of\\nweights, including image classification in computer vision (CV) models,\\nscientific machine learning (SciML) model training, and large language model\\n(LLM) pruning. Our results show that despite its simplicity, FARMS uniformly\\nimproves the accuracy of eigenspectrum analysis while enabling more effective\\nlayer-wise hyperparameter assignment in these application domains. In one of\\nthe LLM pruning experiments, FARMS reduces the perplexity of the LLaMA-7B model\\nby 17.3% when compared with the state-of-the-art method.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Current LLM unlearning methods are not robust: they can be reverted easily\\nwith a few steps of finetuning. This is true even for the idealized unlearning\\nmethod of training to imitate an oracle model that was never exposed to\\nunwanted information, suggesting that output-based finetuning is insufficient\\nto achieve robust unlearning. In a similar vein, we find that training a\\nrandomly initialized student to imitate an unlearned model transfers desired\\nbehaviors while leaving undesired capabilities behind. In other words,\\ndistillation robustifies unlearning. Building on this insight, we propose\\nUnlearn-Noise-Distill-on-Outputs (UNDO), a scalable method that distills an\\nunlearned model into a partially noised copy of itself. UNDO introduces a\\ntunable tradeoff between compute cost and robustness, establishing a new Pareto\\nfrontier on synthetic language and arithmetic tasks. At its strongest setting,\\nUNDO matches the robustness of a model retrained from scratch with perfect data\\nfiltering while using only 60-80% of the compute and requiring only 0.01% of\\nthe pretraining data to be labeled. We also show that UNDO robustifies\\nunlearning on the more realistic Weapons of Mass Destruction Proxy (WMDP)\\nbenchmark. Since distillation is widely used in practice, incorporating an\\nunlearning step beforehand offers a convenient path to robust capability\\nremoval.   \n",
       "3                                                                                                                                                                                                                                                 Despite recent progress in vision-language models (VLMs), holistic\\nunderstanding of long-form video content remains a significant challenge,\\npartly due to limitations in current benchmarks. Many focus on peripheral,\\n``needle-in-a-haystack'' details, encouraging context-insensitive retrieval\\nover deep comprehension. Others rely on large-scale, semi-automatically\\ngenerated questions (often produced by language models themselves) that are\\neasier for models to answer but fail to reflect genuine understanding. In this\\npaper, we introduce MF$^2$, a new benchmark for evaluating whether models can\\ncomprehend, consolidate, and recall key narrative information from full-length\\nmovies (50-170 minutes long). MF$^2$ includes over 50 full-length,\\nopen-licensed movies, each paired with manually constructed sets of claim pairs\\n-- one true (fact) and one plausible but false (fib), totalling over 850 pairs.\\nThese claims target core narrative elements such as character motivations and\\nemotions, causal chains, and event order, and refer to memorable moments that\\nhumans can recall without rewatching the movie. Instead of multiple-choice\\nformats, we adopt a binary claim evaluation protocol: for each pair, models\\nmust correctly identify both the true and false claims. This reduces biases\\nlike answer ordering and enables a more precise assessment of reasoning. Our\\nexperiments demonstrate that both open-weight and closed state-of-the-art\\nmodels fall well short of human performance, underscoring the relative ease of\\nthe task for humans and their superior ability to retain and reason over\\ncritical narrative information -- an ability current VLMs lack.   \n",
       "4                                                                                                                                                                                                         The properties of satellite halos provide a promising probe for dark matter\\n(DM) physics. Observations motivate current efforts to explain surprisingly\\ncompact DM halos. If DM is not collisionless but has strong self-interactions,\\nhalos can undergo gravothermal collapse, leading to higher densities in the\\ncentral region of the halo. However, it is challenging to model this collapse\\nphase from first principles. To improve on this, we seek to better understand\\nnumerical challenges and convergence properties of self-interacting dark matter\\n(SIDM) N-body simulations in the collapse phase. Especially we aim for a better\\nunderstanding of the evolution of satellite halos. To do so, we run SIDM N-body\\nsimulations of a low mass halo in isolation and within an external\\ngravitational potential. The simulation setup is motivated by the perturber of\\nthe stellar stream GD-1. We find that the halo evolution is very sensitive to\\nenergy conservation errors, and a too large SIDM kernel size can artificially\\nspeed up the collapse. Moreover, we demonstrate that the King model can\\ndescribe the density profile at small radii for the late stages that we have\\nsimulated. Furthermore, for our highest-resolved simulation (N = 5x10^7) we\\nmake the data public. It can serve as a benchmark. Overall, we find that the\\ncurrent numerical methods do not suffer from convergence problems in the late\\ncollapse phase and provide guidance on how to choose numerical parameters, e.g.\\nthat the energy conservation error is better kept well below 1%. This allows to\\nrun simulations of halos becoming concentrated enough to explain observations\\nof GD-1 like stellar streams or strong gravitational lensing systems.   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                     Large language models are often used to answer queries grounded in large text\\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\\nentire corpus in the context window and leveraging in-context learning (ICL).\\nAlthough current models support contexts of 100K-1M tokens, this setup is\\ncostly to serve because the memory consumption of the KV cache scales with\\ninput length. We explore an alternative: training a smaller KV cache offline on\\neach corpus. At inference time, we load this trained KV cache, which we call a\\nCartridge, and decode a response. Critically, the cost of training a Cartridge\\ncan be amortized across all the queries referencing the same corpus. However,\\nwe find that the naive approach of training the Cartridge with next-token\\nprediction on the corpus is not competitive with ICL. Instead, we propose\\nself-study, a training recipe in which we generate synthetic conversations\\nabout the corpus and train the Cartridge with a context-distillation objective.\\nWe find that Cartridges trained with self-study replicate the functionality of\\nICL, while being significantly cheaper to serve. On challenging long-context\\nbenchmarks, Cartridges trained with self-study match ICL performance while\\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\\nextends the model's effective context length (e.g. from 128k to 484k tokens on\\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\\ntime without retraining.   \n",
       "6                                                                             Spiking Neural Networks (SNNs) event-driven nature enables efficient encoding\\nof spatial and temporal features, making them suitable for dynamic\\ntime-dependent data processing. Despite their biological relevance, SNNs have\\nseen limited application in medical image recognition due to difficulties in\\nmatching the performance of conventional deep learning models. To address this,\\nwe propose a novel breast cancer classification approach that combines SNNs\\nwith Lempel-Ziv Complexity (LZC) a computationally efficient measure of\\nsequence complexity. LZC enhances the interpretability and accuracy of\\nspike-based models by capturing structural patterns in neural activity. Our\\nstudy explores both biophysical Leaky Integrate-and-Fire (LIF) and\\nprobabilistic Levy-Baxter (LB) neuron models under supervised, unsupervised,\\nand hybrid learning regimes. Experiments were conducted on the Breast Cancer\\nWisconsin dataset using numerical features derived from medical imaging.\\nLB-based models consistently exceeded 90.00% accuracy, while LIF-based models\\nreached over 85.00%. The highest accuracy of 98.25% was achieved using an\\nANN-to-SNN conversion method applied to both neuron models comparable to\\ntraditional deep learning with back-propagation, but at up to 100 times lower\\ncomputational cost. This hybrid approach merges deep learning performance with\\nthe efficiency and plausibility of SNNs, yielding top results at lower\\ncomputational cost. We hypothesize that the synergy between temporal-coding,\\nspike-sparsity, and LZC-driven complexity analysis enables more-efficient\\nfeature extraction. Our findings demonstrate that SNNs combined with LZC offer\\npromising, biologically plausible alternative to conventional neural networks\\nin medical diagnostics, particularly for resource-constrained or real-time\\nsystems.   \n",
       "7                                                                                                                                                                                                                                   Ensuring the safety and certifiability of autonomous surface vessels (ASVs)\\nrequires robust decision-making systems, supported by extensive simulation,\\ntesting, and validation across a broad range of scenarios. However, the current\\nlandscape of maritime autonomy development is fragmented -- relying on\\ndisparate tools for communication, simulation, monitoring, and system\\nintegration -- which hampers interdisciplinary collaboration and inhibits the\\ncreation of compelling assurance cases, demanded by insurers and regulatory\\nbodies. Furthermore, these disjointed tools often suffer from performance\\nbottlenecks, vendor lock-in, and limited support for continuous integration\\nworkflows. To address these challenges, we introduce PyGemini, a permissively\\nlicensed, Python-native framework that builds on the legacy of Autoferry Gemini\\nto unify maritime autonomy development. PyGemini introduces a novel\\nConfiguration-Driven Development (CDD) process that fuses Behavior-Driven\\nDevelopment (BDD), data-oriented design, and containerization to support\\nmodular, maintainable, and scalable software architectures. The framework\\nfunctions as a stand-alone application, cloud-based service, or embedded\\nlibrary -- ensuring flexibility across research and operational contexts. We\\ndemonstrate its versatility through a suite of maritime tools -- including 3D\\ncontent generation for simulation and monitoring, scenario generation for\\nautonomy validation and training, and generative artificial intelligence\\npipelines for augmenting imagery -- thereby offering a scalable, maintainable,\\nand performance-oriented foundation for future maritime robotics and autonomy\\nresearch.   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Offline reinforcement learning (RL) is crucial when online exploration is\\ncostly or unsafe but often struggles with high epistemic uncertainty due to\\nlimited data. Existing methods rely on fixed conservative policies, restricting\\nadaptivity and generalization. To address this, we propose Reflect-then-Plan\\n(RefPlan), a novel doubly Bayesian offline model-based (MB) planning approach.\\nRefPlan unifies uncertainty modeling and MB planning by recasting planning as\\nBayesian posterior estimation. At deployment, it updates a belief over\\nenvironment dynamics using real-time observations, incorporating uncertainty\\ninto MB planning via marginalization. Empirical results on standard benchmarks\\nshow that RefPlan significantly improves the performance of conservative\\noffline RL policies. In particular, RefPlan maintains robust performance under\\nhigh epistemic uncertainty and limited data, while demonstrating resilience to\\nchanging environment dynamics, improving the flexibility, generalizability, and\\nrobustness of offline-learned policies.   \n",
       "9                                                                                                                                                                                                         Bandeira et al. (2022) introduced the Franz-Parisi (FP) criterion for\\ncharacterizing the computational hard phases in statistical detection problems.\\nThe FP criterion, based on an annealed version of the celebrated Franz-Parisi\\npotential from statistical physics, was shown to be equivalent to low-degree\\npolynomial (LDP) lower bounds for Gaussian additive models, thereby connecting\\ntwo distinct approaches to understanding the computational hardness in\\nstatistical inference. In this paper, we propose a refined FP criterion that\\naims to better capture the geometric ``overlap\" structure of statistical\\nmodels. Our main result establishes that this optimized FP criterion is\\nequivalent to Statistical Query (SQ) lower bounds -- another foundational\\nframework in computational complexity of statistical inference. Crucially, this\\nequivalence holds under a mild, verifiable assumption satisfied by a broad\\nclass of statistical models, including Gaussian additive models, planted sparse\\nmodels, as well as non-Gaussian component analysis (NGCA), single-index (SI)\\nmodels, and convex truncation detection settings. For instance, in the case of\\nconvex truncation tasks, the assumption is equivalent with the Gaussian\\ncorrelation inequality (Royen, 2014) from convex geometry.\\n  In addition to the above, our equivalence not only unifies and simplifies the\\nderivation of several known SQ lower bounds -- such as for the NGCA model\\n(Diakonikolas et al., 2017) and the SI model (Damian et al., 2024) -- but also\\nyields new SQ lower bounds of independent interest, including for the\\ncomputational gaps in mixed sparse linear regression (Arpino et al., 2023) and\\nconvex truncation (De et al., 2023).   \n",
       "\n",
       "                                               categories  \n",
       "0                                                 [cs.CV]  \n",
       "1                                          [cs.LG, cs.AI]  \n",
       "2                                          [cs.LG, cs.AI]  \n",
       "3                                   [cs.CV, cs.CL, cs.LG]  \n",
       "4                      [astro-ph.CO, astro-ph.GA, hep-ph]  \n",
       "5                                   [cs.CL, cs.AI, cs.LG]  \n",
       "6                              [cs.NE, eess.IV, q-bio.NC]  \n",
       "7    [cs.RO, cs.SE, cs.SY, eess.SY, D.2.11; I.6.2; I.2.9]  \n",
       "8                                          [cs.AI, cs.LG]  \n",
       "9  [math.ST, cond-mat.stat-mech, cs.CC, stat.ML, stat.TH]  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query To Fetch AI-Related Papers ??\n",
    "query = 'ai OR artificial intelligence OR machine learning'\n",
    "\n",
    "search = arxiv.Search(query=query, max_results=10, sort_by=arxiv.SortCriterion.SubmittedDate)\n",
    "\n",
    "# Fetch Papers ??\n",
    "papers = []\n",
    "\n",
    "for result in search.results():\n",
    "    papers.append({\n",
    "        'published': result.published,\n",
    "        'title': result.title,\n",
    "        'abstract': result.summary,\n",
    "        'categories': result.categories,\n",
    "    })\n",
    "\n",
    "# Convert To DataFrame ??\n",
    "df = pd.DataFrame(papers)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f88b050c-eefb-4249-ae83-2c2ff3ec7ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1ce4ec1d9f481e99d1fc8c622170ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hp\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fffbf3f72a649eeb501e40bf4e73163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example abstract from API ??\n",
    "abstract = df['abstract'][0]\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Summarization ??\n",
    "summarization_result = summarizer(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcbca6b-b7b9-4e71-a20f-9b6b166b0c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_result[0]['summary_text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
