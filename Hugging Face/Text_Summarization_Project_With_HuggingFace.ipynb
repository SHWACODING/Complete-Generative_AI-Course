{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a820b2-3cf1-4b9d-9610-d1ac4c04ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad772e45-bf5f-4600-a3bd-5016d5eb34fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5771f609-d6b3-47fe-9fe7-465cf955b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2a8dfa-19c4-400b-8fd3-6aaade803bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8de149-7265-4414-980c-0fd4f5a4e647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_samsum = load_dataset(\"knkarthick/samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022c0f77-6ec9-4d41-b491-6c5338f505d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_samsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0436f3-e29c-4fc0-a51c-a6abb93a8668",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_samsum['train']['dialogue'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dfe237-ca0a-4f56-a360-44558631588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_samsum['train'][1]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c340d3ff-3180-4b31-8746-a54e15756018",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_lengths = [len(dataset_samsum[split]) for split in dataset_samsum]\n",
    "\n",
    "print(f\"Split Lengths => {split_lengths}\")\n",
    "print(f\"Features => {dataset_samsum['train'].column_names}\")\n",
    "\n",
    "print(\"\\nDialogue => \")\n",
    "print(dataset_samsum['test'][1]['dialogue'])\n",
    "\n",
    "print(\"\\nSummary => \")\n",
    "print(dataset_samsum[\"test\"][1][\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fc91ec-fa97-445b-a24b-53d1ce8bb333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features (example_batch):\n",
    "    input_encodings = tokenizer(example_batch['dialogue'], max_length=1024, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch['summary'], max_length=128, truncation=True)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c0c101-2d34-4bba-a5e1-6b769da84e48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a640507-b0d6-43e2-a52d-947117bd0090",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_samsum_pt['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6327e0-14ce-42e0-9ccb-e7f01b980f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_samsum_pt['train']['input_ids'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7ea67f-c1fc-4b5f-8c4a-158a245a6294",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_samsum_pt['train']['attention_mask'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4011293a-3410-47c5-9f2e-b2f445e2f0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_samsum_pt['train']['labels'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc698c-1a8f-4c4b-9791-b6767449dc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training ??\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73cfb35-8721-4a43-919e-c049c9f8c0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir='pegasus_samsum',\n",
    "    num_train_epochs=1,\n",
    "    warmup_steps=500,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_szie=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    save_steps=1e6,\n",
    "    gradient_accumulation_steps=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d2a3d3-cb95-4bd7-839d-6b470c2a12a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_pegasus,\n",
    "    args=trainer_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=seq2seq_data_collator,\n",
    "    train_dataset=dataset_samsum_pt['train'],\n",
    "    eval_dataset=dataset_samsum_pt['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5411a839-c329-4be5-9ced-ba4502e0d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb242ac-c8dd-4208-8c6e-37f75ae64c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
    "    \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
    "    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "\n",
    "\n",
    "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n",
    "                               batch_size=16, device=device,\n",
    "                               column_text=\"article\",\n",
    "                               column_summary=\"highlights\"):\n",
    "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "    for article_batch, target_batch in tqdm(\n",
    "        zip(article_batches, target_batches), total=len(article_batches)):\n",
    "\n",
    "        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n",
    "                        padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                         attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                         length_penalty=0.8, num_beams=8, max_length=128)\n",
    "        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n",
    "\n",
    "        # Finally, we decode the generated texts,\n",
    "        # replace the  token, and add the decoded texts with the references to the metric.\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
    "                                clean_up_tokenization_spaces=True)\n",
    "               for s in summaries]\n",
    "\n",
    "        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
    "\n",
    "\n",
    "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "    #  Finally compute and return the ROUGE scores.\n",
    "    score = metric.compute()\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38daa93b-a216-475e-ab8c-c910fb938570",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "rouge_metric = load_metric('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590b17ae-1c86-45d0-8fc1-04fec0302be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "model_pegasus.save_pretrained(\"pegasus-samsum-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfb3068-e45b-49bf-b88d-bf95e646da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save tokenizer\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0dfa64-9692-4963-8d8f-6583a05ab38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5474b-1ee5-4d65-a704-74bbd3ae5599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "\n",
    "gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n",
    "\n",
    "\n",
    "\n",
    "sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n",
    "\n",
    "reference = dataset_samsum[\"test\"][0][\"summary\"]\n",
    "\n",
    "pipe = pipeline(\"summarization\", model=\"pegasus-samsum-model\",tokenizer=tokenizer)\n",
    "\n",
    "##\n",
    "print(\"Dialogue:\")\n",
    "print(sample_text)\n",
    "\n",
    "\n",
    "print(\"\\nReference Summary:\")\n",
    "print(reference)\n",
    "\n",
    "\n",
    "print(\"\\nModel Summary:\")\n",
    "print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47adb706-37b6-4de6-8205-1719a4f6a66b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
